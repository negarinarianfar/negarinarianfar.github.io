<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>MiniChess-RL | Negar Arianfar</title>

<link rel="stylesheet" href="/assets/style.css">
<div class="back-link">
← <a href="../index.html">Home</a>
</div>
</head>

<body>

<div class="page">


<div class="project-header">
<h2>MiniChess-RL: Deep Q-Network Agent in a Custom 4×4 Chess Environment</h2>

<p class="project-subtitle">
MiniChess-RL is a fully custom reinforcement learning environment where a Deep Q-Network
agent is trained end-to-end in a 4×4 chess setting. The project focuses on
environment modeling, reward shaping, illegal action masking, and structured evaluation
with reproducible experiments.
</p>

<div class="links">
<a class="btn" href="https://github.com/negarinarianfar/mini-chess-rl" target="_blank">
GitHub Repository
</a>
</div>
</div>


<div class="card">
<h2>Research Question</h2>
<p>
How do reward design choices and environment modeling decisions affect
learning stability and measurable performance in a compact chess-like MDP
with a large discrete action space?
</p>
</div>


<div class="card">
<h2>System Design</h2>
<ul>
<li>Custom Gym-style environment with full MDP formalization</li>
<li>State encoding: 4-channel tensor (WK, WR, BK, turn)</li>
<li>Action space: 256 discrete actions (from × to)</li>
<li>DQN with replay buffer and target network stabilization</li>
<li>Epsilon-greedy exploration strategy</li>
<li>Evaluation pipeline with reward-curve visualization</li>
</ul>
</div>


<div class="card">
<h2>Experimental Results</h2>

<div class="kpi-grid">

<div class="kpi-box">
<div class="kpi-label">Training Episodes</div>
<div class="kpi-value">2000</div>
</div>

<div class="kpi-box">
<div class="kpi-label">Win Rate vs Random</div>
<div class="kpi-value">19%</div>
</div>

<div class="kpi-box">
<div class="kpi-label">Average Reward (100 eval episodes)</div>
<div class="kpi-value">-0.588</div>
</div>

</div>

<p style="margin-top:15px;">
Results demonstrate partial learning behavior and highlight the sensitivity of
reward shaping and exploration scheduling in compact adversarial environments.
</p>

<img class="training-curve"
src="/assets/minichess_reward_curve.png"
alt="MiniChess RL Reward Curve">

</div>


<div class="card">
<h2>Reproducibility</h2>

<p><strong>Train:</strong></p>
<code>python3 training/train.py</code>

<p style="margin-top:12px;"><strong>Evaluate:</strong></p>
<code>python3 training/evaluate.py</code>

<p style="margin-top:15px;">
Experiments were conducted using PyTorch with a fixed evaluation protocol
and greedy policy testing.
</p>

</div>


<div class="card">
<h2>Future Directions</h2>
<ul>
<li>Reward shaping ablation studies</li>
<li>Multiple seed evaluation for variance analysis</li>
<li>Self-play extension and stronger baselines</li>
<li>Architectural comparison (Double DQN / Dueling DQN)</li>
</ul>
</div>


<div class="tags">
<span class="tag">Reinforcement Learning</span>
<span class="tag">Deep Q-Network</span>
<span class="tag">Environment Design</span>
<span class="tag">Illegal Action Masking</span>
<span class="tag">Evaluation</span>
<span class="tag">PyTorch</span>
</div>

</div>
  

  

</body>
</html>
