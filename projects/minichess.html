<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>MiniChess-RL | Negar Arianfar</title>

<link rel="stylesheet" href="/style.css">

<style>
body {
  margin: 0;
}

.page {
  max-width: 1000px;
  margin: 90px auto;
  padding: 0 20px 60px;
}

.back-link {
  opacity: 0.7;
  font-size: 14px;
  margin-bottom: 20px;
}

.back-link a {
  text-decoration: none;
}

.project-header h1 {
  margin-bottom: 10px;
}

.project-subtitle {
  opacity: 0.85;
  max-width: 750px;
  line-height: 1.6;
}

.links {
  margin-top: 15px;
}

.btn {
  display: inline-block;
  padding: 8px 14px;
  border-radius: 10px;
  border: 1px solid rgba(255,255,255,0.2);
  text-decoration: none;
  margin-right: 10px;
  transition: 0.25s ease;
}

.btn:hover {
  transform: translateY(-2px);
  border-color: rgba(0,140,255,0.6);
}

.card {
  background: rgba(255,255,255,0.03);
  border: 1px solid rgba(255,255,255,0.08);
  padding: 22px;
  border-radius: 16px;
  margin-top: 25px;
}

.card h2 {
  margin-top: 0;
}

.kpi-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
  gap: 15px;
  margin-top: 15px;
}

.kpi-box {
  border: 1px solid rgba(255,255,255,0.1);
  border-radius: 14px;
  padding: 14px;
  background: rgba(0,0,0,0.2);
}

.kpi-label {
  font-size: 13px;
  opacity: 0.7;
}

.kpi-value {
  font-size: 20px;
  margin-top: 6px;
  font-weight: bold;
}

img.training-curve {
  max-width: 100%;
  border-radius: 14px;
  margin-top: 15px;
  border: 1px solid rgba(255,255,255,0.1);
}

.tags {
  margin-top: 20px;
}

.tag {
  display: inline-block;
  padding: 5px 10px;
  border-radius: 999px;
  border: 1px solid rgba(255,255,255,0.15);
  font-size: 13px;
  margin-right: 8px;
  margin-bottom: 8px;
}
</style>
</head>

<body>

<div class="page">

<div class="back-link">
← <a href="/projects/">Back to Projects</a>
</div>

<div class="project-header">
<h1>MiniChess-RL: Deep Q-Network Agent in a Custom 4×4 Chess Environment</h1>

<p class="project-subtitle">
MiniChess-RL is a fully custom reinforcement learning environment where a Deep Q-Network
agent is trained end-to-end in a 4×4 chess setting. The project focuses on
environment modeling, reward shaping, illegal action masking, and structured evaluation
with reproducible experiments.
</p>

<div class="links">
<a class="btn" href="https://github.com/negarinarianfar/mini-chess-rl" target="_blank">
GitHub Repository
</a>
</div>
</div>


<div class="card">
<h2>Research Question</h2>
<p>
How do reward design choices and environment modeling decisions affect
learning stability and measurable performance in a compact chess-like MDP
with a large discrete action space?
</p>
</div>


<div class="card">
<h2>System Design</h2>
<ul>
<li>Custom Gym-style environment with full MDP formalization</li>
<li>State encoding: 4-channel tensor (WK, WR, BK, turn)</li>
<li>Action space: 256 discrete actions (from × to)</li>
<li>DQN with replay buffer and target network stabilization</li>
<li>Epsilon-greedy exploration strategy</li>
<li>Evaluation pipeline with reward-curve visualization</li>
</ul>
</div>


<div class="card">
<h2>Experimental Results</h2>

<div class="kpi-grid">

<div class="kpi-box">
<div class="kpi-label">Training Episodes</div>
<div class="kpi-value">2000</div>
</div>

<div class="kpi-box">
<div class="kpi-label">Win Rate vs Random</div>
<div class="kpi-value">19%</div>
</div>

<div class="kpi-box">
<div class="kpi-label">Average Reward (100 eval episodes)</div>
<div class="kpi-value">-0.588</div>
</div>

</div>

<p style="margin-top:15px;">
Results demonstrate partial learning behavior and highlight the sensitivity of
reward shaping and exploration scheduling in compact adversarial environments.
</p>

<img class="training-curve"
src="/assets/minichess_reward_curve.png"
alt="MiniChess RL Reward Curve">

</div>


<div class="card">
<h2>Reproducibility</h2>

<p><strong>Train:</strong></p>
<code>python3 training/train.py</code>

<p style="margin-top:12px;"><strong>Evaluate:</strong></p>
<code>python3 training/evaluate.py</code>

<p style="margin-top:15px;">
Experiments were conducted using PyTorch with a fixed evaluation protocol
and greedy policy testing.
</p>

</div>


<div class="card">
<h2>Future Directions</h2>
<ul>
<li>Reward shaping ablation studies</li>
<li>Multiple seed evaluation for variance analysis</li>
<li>Self-play extension and stronger baselines</li>
<li>Architectural comparison (Double DQN / Dueling DQN)</li>
</ul>
</div>


<div class="tags">
<span class="tag">Reinforcement Learning</span>
<span class="tag">Deep Q-Network</span>
<span class="tag">Environment Design</span>
<span class="tag">Illegal Action Masking</span>
<span class="tag">Evaluation</span>
<span class="tag">PyTorch</span>
</div>

</div>

</body>
</html>
